{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install transformers\\n!pip install torch\\n!pip install -U PyPDF2\\n!pip install python-docx'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!pip install transformers\n",
    "!pip install torch\n",
    "!pip install -U PyPDF2\n",
    "!pip install python-docx'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Reading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for reading PDF, Docx, txt or a whole directory. \n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "def read_word(file_path): \n",
    "    doc = docx.Document(file_path)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_txt(file_path): \n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def read_documents_from_directory(directory):\n",
    "    combined_text = \"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            combined_text += read_pdf(file_path)\n",
    "        elif filename.endswith(\".docx\"):\n",
    "            combined_text += read_word(file_path)\n",
    "        elif filename.endswith(\".txt\"):\n",
    "            combined_text += read_txt(file_path)\n",
    "    return combined_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_chatbot(directory, model_output_path, train_fraction=0.8):\n",
    "    # Read documents from the directory\n",
    "    combined_text = read_documents_from_directory(directory)\n",
    "    combined_text = re.sub(r'\\n+', '\\n', combined_text).strip()  # Remove excess newline characters\n",
    "\n",
    "    # Split the text into training and validation sets\n",
    "    split_index = int(train_fraction * len(combined_text))\n",
    "    train_text = combined_text[:split_index]\n",
    "    val_text = combined_text[split_index:]\n",
    "\n",
    "    # Save the training and validation data as text files\n",
    "    with open(\"train.txt\", \"w\") as f:\n",
    "        f.write(train_text)\n",
    "    with open(\"val.txt\", \"w\") as f:\n",
    "        f.write(val_text)\n",
    "\n",
    "    # Tokenizer and Model Preparation\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")  #Setting tokenizer for GP2 Large\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")  #Setting the model to GPT2 Large\n",
    "\n",
    "    # Dataset Preparation\n",
    "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"train.txt\", block_size=128)\n",
    "    val_dataset = TextDataset(tokenizer=tokenizer, file_path=\"val.txt\", block_size=128)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_path,\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=30,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_output_path)\n",
    "    \n",
    "    # Save the tokenizer\n",
    "    tokenizer.save_pretrained(model_output_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Answering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=100):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create the attention mask and pad token id\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Main Function for the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    directory = \"Documents\"  # Replace with the path to your directory containing the files\n",
    "    model_output_path = \"ModelOutput\"\n",
    "\n",
    "    # Train the chatbot\n",
    "    train_chatbot(directory, model_output_path)\n",
    "\n",
    "    # Load the fine-tuned model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_output_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_output_path)\n",
    "\n",
    "    # Test the chatbot\n",
    "    prompt = \"Norma Boliviana\"  # PROMPT! Introduce the Prompt Here!\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    print(\"Generated response:\", response)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JoseLuis\\anaconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the  Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "c:\\Users\\JoseLuis\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2df4af1114470ebdd73b8af6c0729a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6778, 'learning_rate': 4.759152215799615e-05, 'epoch': 1.45}\n",
      "{'loss': 2.1097, 'learning_rate': 4.518304431599229e-05, 'epoch': 2.89}\n",
      "{'loss': 1.7656, 'learning_rate': 4.2774566473988445e-05, 'epoch': 4.34}\n",
      "{'loss': 1.4837, 'learning_rate': 4.036608863198459e-05, 'epoch': 5.78}\n",
      "{'loss': 1.2201, 'learning_rate': 3.7957610789980736e-05, 'epoch': 7.23}\n",
      "{'loss': 0.9925, 'learning_rate': 3.554913294797688e-05, 'epoch': 8.67}\n",
      "{'loss': 0.7999, 'learning_rate': 3.314065510597303e-05, 'epoch': 10.12}\n",
      "{'loss': 0.6209, 'learning_rate': 3.073217726396917e-05, 'epoch': 11.56}\n",
      "{'loss': 0.512, 'learning_rate': 2.832369942196532e-05, 'epoch': 13.01}\n",
      "{'loss': 0.3946, 'learning_rate': 2.5915221579961463e-05, 'epoch': 14.45}\n",
      "{'loss': 0.3333, 'learning_rate': 2.3506743737957612e-05, 'epoch': 15.9}\n",
      "{'loss': 0.2658, 'learning_rate': 2.1098265895953757e-05, 'epoch': 17.34}\n",
      "{'loss': 0.225, 'learning_rate': 1.8689788053949906e-05, 'epoch': 18.79}\n",
      "{'loss': 0.1947, 'learning_rate': 1.628131021194605e-05, 'epoch': 20.23}\n",
      "{'loss': 0.1676, 'learning_rate': 1.3872832369942197e-05, 'epoch': 21.68}\n",
      "{'loss': 0.1493, 'learning_rate': 1.1464354527938344e-05, 'epoch': 23.12}\n",
      "{'loss': 0.1327, 'learning_rate': 9.05587668593449e-06, 'epoch': 24.57}\n",
      "{'loss': 0.1223, 'learning_rate': 6.647398843930635e-06, 'epoch': 26.01}\n",
      "{'loss': 0.1109, 'learning_rate': 4.238921001926782e-06, 'epoch': 27.46}\n",
      "{'loss': 0.104, 'learning_rate': 1.8304431599229288e-06, 'epoch': 28.9}\n",
      "{'train_runtime': 2751.0593, 'train_samples_per_second': 15.071, 'train_steps_per_second': 3.773, 'train_loss': 0.6964291041526721, 'epoch': 30.0}\n",
      "Generated response: Norma Boliviana     NB 777\n",
      "Instituto Boliviano de Normalizaci贸n y Calidad\n",
      "Dise帽o y construcci贸n \n",
      "de instalaciones \n",
      "el茅ctricas interiores en \n",
      "baja tensi贸n \n",
      "Primera revisi贸n \n",
      " \n",
      "ICS 91.140.50  Sistemas de suministro de electricidad \n",
      " \n",
      "Diciembre 2007 \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Using the trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=250):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create the attention mask and pad token id\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ModelOutput\"\n",
    "# Load the fine-tuned model and tokenizer\n",
    "my_chat_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "my_chat_tokenizer = GPT2Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: Factor de potencia \n",
      "Es la relaci贸n entre la demanda m谩xima y la potencia total instalada para satisfacer est谩 \n",
      "demanda, es valido para un determinado punto y per铆odo de tiempo. \n",
      " \n",
      "2.74 Falla \n",
      " \n",
      "Uni贸n entre dos puntos a potencial diferente o ausencia temporal o permanente de la \n",
      "energ铆a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Factor de potencia\"  # Replace with your desired prompt\n",
    "#prompt = \"What is the most promising future technology?\"\n",
    "response = generate_response(my_chat_model, my_chat_tokenizer, prompt, max_length=100)  #\n",
    "print(\"Generated response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
